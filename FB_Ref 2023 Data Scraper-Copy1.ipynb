{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90697d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Package Imports and CSV Reads\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "big5_squad_codes = pd.read_csv('big5_squad_codes.csv')\n",
    "mls_squad_codes = pd.read_csv('mls_squad_codes.csv')\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddd25340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_and_save(links, columns, df, file_name):\n",
    "    i = 0\n",
    "    # Loop through each link\n",
    "    for link in tqdm.tqdm(links):\n",
    "        time.sleep(3)\n",
    "        \n",
    "        try:\n",
    "            # Use requests to get the HTML content of the page\n",
    "            res = requests.get(links[i])\n",
    "            soup = BeautifulSoup(res.text, 'html.parser')\n",
    "            i += 1\n",
    "            # Find the header for team info - this is different than the rest of the stats categories\n",
    "            inner_nav = soup.find('div', id='inner_nav')\n",
    "            header_data = inner_nav.find('a').text\n",
    "\n",
    "            # Find the table with the specified div id\n",
    "            table = soup.find('table', {'id': 'matchlogs_for'})\n",
    "\n",
    "            # Find table body\n",
    "            table_body = table.find('tbody')\n",
    "\n",
    "            # Find all the rows in the table\n",
    "            rows = table_body.find_all('tr')\n",
    "\n",
    "            # Initialize an empty list to store the data\n",
    "            data = []\n",
    "\n",
    "            # Loop through each row and extract the data\n",
    "            for row in rows:\n",
    "                cols = row.find_all(['td','th'])\n",
    "                cols = [col.text.strip() for col in cols]\n",
    "                data.append(cols)\n",
    "\n",
    "            # Create a temporary dataframe from the extracted data\n",
    "            temp_df = pd.DataFrame(data, columns=columns)\n",
    "            temp_df['Team'] = header_data\n",
    "            df = df.append(temp_df, ignore_index=True)\n",
    "            \n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # Save the DataFrame as a CSV file\n",
    "    df.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f4e3ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lists to loop through\n",
    "cats = ['schedule','shooting','keeper','passing','passing_types','gca','defense','possession','misc']\n",
    "year_range = ['2017-2018','2018-2019','2019-2020','2020-2021','2021-2022','2022-2023']\n",
    "mls_year_range = ['2018','2019','2020','2021','2022']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47db556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FB Ref Columns to create dataframes. \n",
    "\n",
    "big_5_score_columns = [['Date','Time','Comp','Round','Day','Venue','Result','GF','GA','Opponent','xG','xGA','Possession%','Attendance','Captain','Formation','Referee','Match Report','Notes']]\n",
    "\n",
    "big_5_shooting_columns = [['Date','Time','Comp','Round','Day','Venue','Result','GF','GA','Opponent','Gls','Sh','SoT','SoT%','G/Sh','G/SoT','Avg Shot Distance','Free Kick Shots','Penalties Made','PKatt','xG','Non pen xG','npxG/Sh','G minus xG','npxG-xG','Match Report']]\n",
    "\n",
    "big_5_goalkeeping_columns = [['Date','Time','Comp','Round','Day','Venue','Result','GF','GA','Opponent','SoTA','GA','Saves','Save%','Clean Sheet','Post-ShotxG','PSxGPlusMinus','PKatt','PKAllowed','PKsaved','PKmissed','Passes40ydspluscompleted','Passes40ydsplusAttempted','Passes40ydsplusCmp%','PassesAtt','PassesThrwn','%ofPassesLaunched','AvgPassLen','GoalKicksAtt','GoalKicks%Launched','GoalKicksAvgLen','Crosses Faced','Crosses Stopped','% of Opp Crosses Stopped','# of Sweeper Actions','AvgDist sweeper Actions','Match Report']]\n",
    "\n",
    "big_5_passing_columns = [['Date','Time','Comp','Round','Day','Venue','Result','GF','GA','Opponent','Passes Completed','Passes Attempted','Pass Completion %','Total Pass Dist','Total Pass Dist Twd Goal','Short Pass Comp','Short Pass Att','Shrt Pass Cmp%','Med Pass Comp','Med Pass Att','Med Pass Cmp %','Lng Pass Cmp','Lng Pass Att','Lng Pass Cmp %','Ast','xAssistedGoals','xAssists','KeyPasses','Attacking Third Passes','Passes Into 18 yd box Comp','Completed Cross','Prog Passes past own 40 yd line','Match Report']]\n",
    "\n",
    "big_5_pass_type_columns = [['Date','Time','Comp','Round','Day','Venue','Result','GF','GA','Opponent','Passes Attempted','Live Passes','Deadball passes','Free Kick Passes','Passes between open defenders','Passes that go 40 plus yds wide','Crosses','Throw Ins','Corner Kicks','Inswinger Corners','Outswinging Corners','Straight Corners','Passes Completed','Offsides','Passes Blocked','Match Report']]\n",
    "\n",
    "big_5_goal_shot_creation_columns = [['Date','Time','Comp','Round','Day','Venue','Result','GF','GA','Opponent','Shot Creating Actions','Live Passes Leading to shot','Deadball Passes Leading to shot','Dribbles leading to shot','Shots leading to another shot','Fouls drawn leading to shot','Def Actions Leading to Shot','Goal Creating Actions','Passes leading to goal','Goals from deadball','Goals from dribble','Goals from other shot','Goals from foul drawn','Goals from defensive action','Match Report']]\n",
    "\n",
    "big_5_defensive_action_columns = [['Date','Time','Comp','Round','Day','Venue','Result','GF','GA','Opponent','Tackles','TacklesWon','Tackles in Def 3rd','Tackles in Mid 3rd','Tackles in Att 3rd','Tackles vs Dribble','Tackles vs Dribble Att','% of Dribblers tackled succesful','# of times dribbled past','Blocks','Shots Blocked','Passes Blocked','Interceptions','Tackles + Int','Clearances','Errors','Match Report']]\n",
    "\n",
    "big_5_possession_columns = [['Date','Time','Comp','Round','Day','Venue','Result','GF','GA','Opponent','Possession','Touches','Touches in Def Pen Area','Touches in Def 3rd','Touches in Mid 3rd','Touches in Att 3rd','Touches in Att Pen Area','Live ball touches','Attempted Take ons','Succesful Take Ons','Take on success rate','Number of times tackled during take on','% of take ons tackled','Carries','Total Carry Distance','Progressive Carrying Distance','Progressive Carries','Carries into final third','Carries into penalty area','Miscontrols','Dispossessions','Passes Received','Progressive Passes Received','Match Report']]\n",
    "\n",
    "big_5_misc_stats_columns = [['Date','Time','Comp','Round','Day','Venue','Result','GF','GA','Opponent','CrdY','CrdR','2CrdY','Fls','Fld','Off','Crs','Int','TklW','PKwon','PKcon','OG','Loose Balls Recov','Aerials Won','Aerials Lost','Aerials Won%','Match Report']]\n",
    "\n",
    "big_5_opp_shooting_columns = [['opp' + x for x in big_5_shooting_columns[0]]]\n",
    "\n",
    "big_5_opp_goalkeeping_columns = [['opp' + x for x in big_5_goalkeeping_columns[0]]]\n",
    "\n",
    "big_5_opp_passing_columns = [['opp' + x for x in big_5_passing_columns[0]]]\n",
    "\n",
    "big_5_opp_pass_type_columns = [['opp' + x for x in big_5_pass_type_columns[0]]]\n",
    "\n",
    "big_5_opp_goal_shot_creation_columns = [['opp' + x for x in big_5_goal_shot_creation_columns[0]]]\n",
    "\n",
    "big_5_opp_defensive_action_columns = [['opp' + x for x in big_5_defensive_action_columns[0]]]\n",
    "\n",
    "big_5_opp_possession_columns = [['opp' + x for x in big_5_possession_columns[0]]]\n",
    "\n",
    "big_5_opp_misc_stats_columns = [['opp' + x for x in big_5_misc_stats_columns[0]]]\n",
    "\n",
    "mls_score_columns = [['Date','Time','Round','Day','Venue','Result','GF','GA','Opponent','xG','xGA','Possession%','Attendance','Captain','Formation','Referee','Match Report','Notes']]\n",
    "\n",
    "mls_shooting_columns = [['Date','Time','Round','Day','Venue','Result','GF','GA','Opponent','Gls','Sh','SoT','SoT%','G/Sh','G/SoT','Avg Shot Distance','Free Kick Shots','Penalties Made','PKatt','xG','Non pen xG','npxG/Sh','G minus xG','npxG-xG','Match Report']]\n",
    "\n",
    "mls_goalkeeping_columns = [['Date','Time','Round','Day','Venue','Result','GF','GA','Opponent','SoTA','GA','Saves','Save%','Clean Sheet','Post-ShotxG','PSxGPlusMinus','PKatt','PKAllowed','PKsaved','PKmissed','Passes40ydspluscompleted','Passes40ydsplusAttempted','Passes40ydsplusCmp%','PassesAtt','PassesThrwn','%ofPassesLaunched','AvgPassLen','GoalKicksAtt','GoalKicks%Launched','GoalKicksAvgLen','OppCrossesAtt','OppCrossesStp','OppCrossesStp%','# of Sweeper Actions','AvgDist sweeper Actions','Match Report']]\n",
    "\n",
    "mls_passing_columns = [['Date','Time','Round','Day','Venue','Result','GF','GA','Opponent','Passes Completed','Passes Attempted','Pass Completion %','Total Pass Dist','Total Pass Dist Twd Goal','Short Pass Comp','Short Pass Att','Shrt Pass Cmp%','Med Pass Comp','Med Pass Att','Med Pass Cmp %','Lng Pass Cmp','Lng Pass Att','Lng Pass Cmp %','Ast','xAssistedGoals','xAssists','KeyPasses','Attacking Third Passes','Passes Into 18 yd box Comp','Completed Cross','Prog Passes past own 40 yd line','Match Report']]\n",
    "\n",
    "mls_pass_type_columns = [['Date','Time','Round','Day','Venue','Result','GF','GA','Opponent','Passes Attempted','Live Passes','Deadball passes','Free Kick Passes','Passes between open defenders','Passes that go 40 plus yds wide','Crosses','Throw Ins','Corner Kicks','Inswinger Corners','Outswinging Corners','Straight Corners','Passes Completed','Offsides','Passes Blocked','Match Report']]\n",
    "\n",
    "mls_goal_shot_creation_columns = [['Date','Time','Round','Day','Venue','Result','GF','GA','Opponent','Shot Creating Actions','Live Passes Leading to shot','Deadball Passes Leading to shot','Dribbles leading to shot','Shots leading to another shot','Fouls drawn leading to shot','Def Actions Leading to Shot','Goal Creating Actions','Passes leading to goal','Goals from deadball','Goals from dribble','Goals from other shot','Goals from foul drawn','Goals from defensive action','Match Report']]\n",
    "\n",
    "mls_defensive_action_columns = [['Date','Time','Round','Day','Venue','Result','GF','GA','Opponent','Tackles','TacklesWon','Tackles in Def 3rd','Tackles in Mid 3rd','Tackles in Att 3rd','Tackles vs Dribble','Tackles vs Dribble Att','% of Dribblers tackled succesful','# of times dribbled past','Blocks','Shots Blocked','Passes Blocked','Interceptions','Tackles + Int','Clearances','Errors','Match Report']]\n",
    "\n",
    "mls_possession_columns = [['Date','Time','Round','Day','Venue','Result','GF','GA','Opponent','Possession','Touches','Touches in Def Pen Area','Touches in Def 3rd','Touches in Mid 3rd','Touches in Att 3rd','Touches in Att Pen Area','Live ball touches','Attempted Take ons','Succesful Take Ons','Take on success rate','Number of times tackled during take on','% of take ons tackled','Carries','Total Carry Distance','Progressive Carrying Distance','Progressive Carries','Carries into final third','Carries into penalty area','Miscontrols','Dispossessions','Passes Received','Progressive Passes Received','Match Report']]\n",
    "\n",
    "mls_misc_stats_columns = [['Date','Time','Round','Day','Venue','Result','GF','GA','Opponent','CrdY','CrdR','2CrdY','Fls','Fld','Off','Crs','Int','TklW','PKwon','PKcon','OG','Loose Balls Recov','Aerials Won','Aerials Lost','Aerials Won%','Match Report']]\n",
    "\n",
    "mls_opp_shooting_columns = [['opp' + x for x in mls_shooting_columns[0]]]\n",
    "\n",
    "mls_opp_goalkeeping_columns = [['opp' + x for x in mls_goalkeeping_columns[0]]]\n",
    "\n",
    "mls_opp_passing_columns = [['opp' + x for x in mls_passing_columns[0]]]\n",
    "\n",
    "mls_opp_pass_type_columns = [['opp' + x for x in mls_pass_type_columns[0]]]\n",
    "\n",
    "mls_opp_goal_shot_creation_columns = [['opp' + x for x in mls_goal_shot_creation_columns[0]]]\n",
    "\n",
    "mls_opp_defensive_action_columns = [['opp' + x for x in mls_defensive_action_columns[0]]]\n",
    "\n",
    "mls_opp_possession_columns = [['opp' + x for x in mls_possession_columns[0]]]\n",
    "\n",
    "mls_opp_misc_stats_columns = [['opp' + x for x in mls_misc_stats_columns[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c03268f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score Links (need to change Match-Logs to Scores-and-Fixtures in link builder.) mls Links are seperate because the tables don't include a competition column. Fbref hates the open cup apparently. \n",
    "big_5_score_links = []\n",
    "mls_score_links = []\n",
    "\n",
    "for year in range(len(year_range)):\n",
    "    for squad in range(len(big5_squad_codes)):\n",
    "        link = (f\"https://fbref.com/en/squads/{big5_squad_codes['Squad_Code'][squad]}/{year_range[year]}/matchlogs/all_comps/{cats[0]}/{big5_squad_codes['Text'][squad]}\")\n",
    "        big_5_score_links.append(link)\n",
    "        \n",
    "for year in range(len(mls_year_range)):\n",
    "    for squad in range(len(mls_squad_codes)):\n",
    "        link = (f\"https://fbref.com/en/squads/{mls_squad_codes['Squad_Code'][squad]}/{mls_year_range[year]}/matchlogs/c22/{cats[0]}/{mls_squad_codes['Text'][squad]}\")\n",
    "        mls_score_links.append(link)\n",
    "    \n",
    "def replace_text(links):\n",
    "    return [s.replace(\"Match-Logs\", \"Scores-and-Fixtures\") for s in links]\n",
    "\n",
    "big_5_score_links = replace_text(big_5_score_links)\n",
    "mls_score_links = replace_text(mls_score_links)\n",
    "\n",
    "# Shooting Links\n",
    "big_5_shooting_links = []\n",
    "mls_shooting_links = []\n",
    "\n",
    "for year in range(len(year_range)):\n",
    "    for squad in range(len(big5_squad_codes)):\n",
    "        link = (f\"https://fbref.com/en/squads/{big5_squad_codes['Squad_Code'][squad]}/{year_range[year]}/matchlogs/all_comps/{cats[1]}/{big5_squad_codes['Text'][squad]}\")\n",
    "        big_5_shooting_links.append(link)\n",
    "        \n",
    "for year in range(len(mls_year_range)):\n",
    "    for squad in range(len(mls_squad_codes)):\n",
    "        link = (f\"https://fbref.com/en/squads/{mls_squad_codes['Squad_Code'][squad]}/{mls_year_range[year]}/matchlogs/c22/{cats[1]}/{mls_squad_codes['Text'][squad]}\")\n",
    "        mls_shooting_links.append(link)\n",
    "        \n",
    "# Goalkeeping Links\n",
    "big_5_goalkeeping_links = []\n",
    "mls_goalkeeping_links = []\n",
    "\n",
    "for year in range(len(year_range)):\n",
    "    for squad in range(len(big5_squad_codes)):\n",
    "        link = (f\"https://fbref.com/en/squads/{big5_squad_codes['Squad_Code'][squad]}/{year_range[year]}/matchlogs/all_comps/{cats[2]}/{big5_squad_codes['Text'][squad]}\")\n",
    "        big_5_goalkeeping_links.append(link)\n",
    "        \n",
    "for year in range(len(mls_year_range)):\n",
    "    for squad in range(len(mls_squad_codes)):\n",
    "        link = (f\"https://fbref.com/en/squads/{mls_squad_codes['Squad_Code'][squad]}/{mls_year_range[year]}/matchlogs/c22/{cats[2]}/{mls_squad_codes['Text'][squad]}\")\n",
    "        mls_goalkeeping_links.append(link)\n",
    "        \n",
    "# Passing Links\n",
    "big_5_passing_links = []\n",
    "mls_passing_links = []\n",
    "\n",
    "for year in range(len(year_range)):\n",
    "    for squad in range(len(big5_squad_codes)):\n",
    "        link = (f\"https://fbref.com/en/squads/{big5_squad_codes['Squad_Code'][squad]}/{year_range[year]}/matchlogs/all_comps/{cats[3]}/{big5_squad_codes['Text'][squad]}\")\n",
    "        big_5_passing_links.append(link)\n",
    "        \n",
    "for year in range(len(mls_year_range)):\n",
    "    for squad in range(len(mls_squad_codes)):\n",
    "        link = (f\"https://fbref.com/en/squads/{mls_squad_codes['Squad_Code'][squad]}/{mls_year_range[year]}/matchlogs/c22/{cats[3]}/{mls_squad_codes['Text'][squad]}\")\n",
    "        mls_passing_links.append(link)\n",
    "        \n",
    "# Pass Type Links\n",
    "big_5_pass_type_links = []\n",
    "mls_pass_type_links = []\n",
    "\n",
    "for year in range(len(year_range)):\n",
    "    for squad in range(len(big5_squad_codes)):\n",
    "        link = (f\"https://fbref.com/en/squads/{big5_squad_codes['Squad_Code'][squad]}/{year_range[year]}/matchlogs/all_comps/{cats[4]}/{big5_squad_codes['Text'][squad]}\")\n",
    "        big_5_pass_type_links.append(link)\n",
    "        \n",
    "for year in range(len(mls_year_range)):\n",
    "    for squad in range(len(mls_squad_codes)):\n",
    "        link = (f\"https://fbref.com/en/squads/{mls_squad_codes['Squad_Code'][squad]}/{mls_year_range[year]}/matchlogs/c22/{cats[4]}/{mls_squad_codes['Text'][squad]}\")\n",
    "        mls_pass_type_links.append(link)\n",
    "        \n",
    "# Goal Shot Creation Links\n",
    "big_5_goal_shot_creation_links = []\n",
    "mls_goal_shot_creation_links = []\n",
    "\n",
    "for year in range(len(year_range)):\n",
    "    for squad in range(len(big5_squad_codes)):\n",
    "        link = (f\"https://fbref.com/en/squads/{big5_squad_codes['Squad_Code'][squad]}/{year_range[year]}/matchlogs/all_comps/{cats[5]}/{big5_squad_codes['Text'][squad]}\")\n",
    "        big_5_goal_shot_creation_links.append(link)\n",
    "        \n",
    "for year in range(len(mls_year_range)):\n",
    "    for squad in range(len(mls_squad_codes)):\n",
    "        link = (f\"https://fbref.com/en/squads/{mls_squad_codes['Squad_Code'][squad]}/{mls_year_range[year]}/matchlogs/c22/{cats[5]}/{mls_squad_codes['Text'][squad]}\")\n",
    "        mls_goal_shot_creation_links.append(link)\n",
    "        \n",
    "# Defensive Action Links\n",
    "big_5_defensive_action_links = []\n",
    "mls_defensive_action_links = []\n",
    "\n",
    "for year in range(len(year_range)):\n",
    "    for squad in range(len(big5_squad_codes)):\n",
    "        link = (f\"https://fbref.com/en/squads/{big5_squad_codes['Squad_Code'][squad]}/{year_range[year]}/matchlogs/all_comps/{cats[6]}/{big5_squad_codes['Text'][squad]}\")\n",
    "        big_5_defensive_action_links.append(link)\n",
    "        \n",
    "for year in range(len(mls_year_range)):\n",
    "    for squad in range(len(mls_squad_codes)):\n",
    "        link = (f\"https://fbref.com/en/squads/{mls_squad_codes['Squad_Code'][squad]}/{mls_year_range[year]}/matchlogs/c22/{cats[6]}/{mls_squad_codes['Text'][squad]}\")\n",
    "        mls_defensive_action_links.append(link)\n",
    "        \n",
    "# Possession Links\n",
    "big_5_possession_links = []\n",
    "mls_possession_links = []\n",
    "\n",
    "for year in range(len(year_range)):\n",
    "    for squad in range(len(big5_squad_codes)):\n",
    "        link = (f\"https://fbref.com/en/squads/{big5_squad_codes['Squad_Code'][squad]}/{year_range[year]}/matchlogs/all_comps/{cats[7]}/{big5_squad_codes['Text'][squad]}\")\n",
    "        big_5_possession_links.append(link)\n",
    "        \n",
    "for year in range(len(mls_year_range)):\n",
    "    for squad in range(len(mls_squad_codes)):\n",
    "        link = (f\"https://fbref.com/en/squads/{mls_squad_codes['Squad_Code'][squad]}/{mls_year_range[year]}/matchlogs/c22/{cats[7]}/{mls_squad_codes['Text'][squad]}\")\n",
    "        mls_possession_links.append(link)\n",
    "        \n",
    "# Misc Links\n",
    "big_5_misc_stats_links = []\n",
    "mls_misc_stats_links = []\n",
    "\n",
    "for year in range(len(year_range)):\n",
    "    for squad in range(len(big5_squad_codes)):\n",
    "        link = (f\"https://fbref.com/en/squads/{big5_squad_codes['Squad_Code'][squad]}/{year_range[year]}/matchlogs/all_comps/{cats[8]}/{big5_squad_codes['Text'][squad]}\")\n",
    "        big_5_misc_stats_links.append(link)\n",
    "        \n",
    "for year in range(len(mls_year_range)):\n",
    "    for squad in range(len(mls_squad_codes)):\n",
    "        link = (f\"https://fbref.com/en/squads/{mls_squad_codes['Squad_Code'][squad]}/{mls_year_range[year]}/matchlogs/c22/{cats[8]}/{mls_squad_codes['Text'][squad]}\")\n",
    "        mls_misc_stats_links.append(link)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "162fc293",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lists to loop through\n",
    "cats = ['schedule','shooting','keeper','passing','passing_types','gca','defense','possession','misc']\n",
    "year_range = ['2022-2023']\n",
    "mls_year_range = ['2023']\n",
    "\n",
    "#2023 Link Creation\n",
    "big_5_score_links_2023 = []\n",
    "mls_score_links_2023 = []\n",
    "\n",
    "for year in range(len(year_range)):\n",
    "    for squad in range(len(big5_squad_codes)):\n",
    "        link = (f\"https://fbref.com/en/squads/{big5_squad_codes['Squad_Code'][squad]}/{year_range[year]}/matchlogs/all_comps/{cats[0]}/{big5_squad_codes['Text'][squad]}\")\n",
    "        big_5_score_links_2023.append(link)\n",
    "        \n",
    "for year in range(len(mls_year_range)):\n",
    "    for squad in range(len(mls_squad_codes)):\n",
    "        link = (f\"https://fbref.com/en/squads/{mls_squad_codes['Squad_Code'][squad]}/{mls_year_range[year]}/matchlogs/c22/{cats[0]}/{mls_squad_codes['Text'][squad]}\")\n",
    "        mls_score_links_2023.append(link)\n",
    "    \n",
    "def replace_text(links):\n",
    "    return [s.replace(\"Match-Logs\", \"Scores-and-Fixtures\") for s in links]\n",
    "\n",
    "big_5_score_links_2023 = replace_text(big_5_score_links_2023)\n",
    "mls_score_links_2023 = replace_text(mls_score_links_2023)\n",
    "\n",
    "# Shooting Links\n",
    "big_5_shooting_links_2023 = []\n",
    "mls_shooting_links_2023 = []\n",
    "\n",
    "for year in range(len(year_range)):\n",
    "    for squad in range(len(big5_squad_codes)):\n",
    "        link = (f\"https://fbref.com/en/squads/{big5_squad_codes['Squad_Code'][squad]}/{year_range[year]}/matchlogs/all_comps/{cats[1]}/{big5_squad_codes['Text'][squad]}\")\n",
    "        big_5_shooting_links_2023.append(link)\n",
    "        \n",
    "for year in range(len(mls_year_range)):\n",
    "    for squad in range(len(mls_squad_codes)):\n",
    "        link = (f\"https://fbref.com/en/squads/{mls_squad_codes['Squad_Code'][squad]}/{mls_year_range[year]}/matchlogs/c22/{cats[1]}/{mls_squad_codes['Text'][squad]}\")\n",
    "        mls_shooting_links_2023.append(link)\n",
    "        \n",
    "# Goalkeeping Links\n",
    "big_5_goalkeeping_links_2023 = []\n",
    "mls_goalkeeping_links_2023 = []\n",
    "\n",
    "for year in range(len(year_range)):\n",
    "    for squad in range(len(big5_squad_codes)):\n",
    "        link = (f\"https://fbref.com/en/squads/{big5_squad_codes['Squad_Code'][squad]}/{year_range[year]}/matchlogs/all_comps/{cats[2]}/{big5_squad_codes['Text'][squad]}\")\n",
    "        big_5_goalkeeping_links_2023.append(link)\n",
    "        \n",
    "for year in range(len(mls_year_range)):\n",
    "    for squad in range(len(mls_squad_codes)):\n",
    "        link = (f\"https://fbref.com/en/squads/{mls_squad_codes['Squad_Code'][squad]}/{mls_year_range[year]}/matchlogs/c22/{cats[2]}/{mls_squad_codes['Text'][squad]}\")\n",
    "        mls_goalkeeping_links_2023.append(link)\n",
    "        \n",
    "# Passing Links\n",
    "big_5_passing_links_2023 = []\n",
    "mls_passing_links_2023 = []\n",
    "\n",
    "for year in range(len(year_range)):\n",
    "    for squad in range(len(big5_squad_codes)):\n",
    "        link = (f\"https://fbref.com/en/squads/{big5_squad_codes['Squad_Code'][squad]}/{year_range[year]}/matchlogs/all_comps/{cats[3]}/{big5_squad_codes['Text'][squad]}\")\n",
    "        big_5_passing_links_2023.append(link)\n",
    "        \n",
    "for year in range(len(mls_year_range)):\n",
    "    for squad in range(len(mls_squad_codes)):\n",
    "        link = (f\"https://fbref.com/en/squads/{mls_squad_codes['Squad_Code'][squad]}/{mls_year_range[year]}/matchlogs/c22/{cats[3]}/{mls_squad_codes['Text'][squad]}\")\n",
    "        mls_passing_links_2023.append(link)\n",
    "        \n",
    "# Pass Type Links\n",
    "big_5_pass_type_links_2023 = []\n",
    "mls_pass_type_links_2023 = []\n",
    "\n",
    "for year in range(len(year_range)):\n",
    "    for squad in range(len(big5_squad_codes)):\n",
    "        link = (f\"https://fbref.com/en/squads/{big5_squad_codes['Squad_Code'][squad]}/{year_range[year]}/matchlogs/all_comps/{cats[4]}/{big5_squad_codes['Text'][squad]}\")\n",
    "        big_5_pass_type_links_2023.append(link)\n",
    "        \n",
    "for year in range(len(mls_year_range)):\n",
    "    for squad in range(len(mls_squad_codes)):\n",
    "        link = (f\"https://fbref.com/en/squads/{mls_squad_codes['Squad_Code'][squad]}/{mls_year_range[year]}/matchlogs/c22/{cats[4]}/{mls_squad_codes['Text'][squad]}\")\n",
    "        mls_pass_type_links_2023.append(link)\n",
    "        \n",
    "# Goal Shot Creation Links\n",
    "big_5_goal_shot_creation_links_2023 = []\n",
    "mls_goal_shot_creation_links_2023 = []\n",
    "\n",
    "for year in range(len(year_range)):\n",
    "    for squad in range(len(big5_squad_codes)):\n",
    "        link = (f\"https://fbref.com/en/squads/{big5_squad_codes['Squad_Code'][squad]}/{year_range[year]}/matchlogs/all_comps/{cats[5]}/{big5_squad_codes['Text'][squad]}\")\n",
    "        big_5_goal_shot_creation_links_2023.append(link)\n",
    "        \n",
    "for year in range(len(mls_year_range)):\n",
    "    for squad in range(len(mls_squad_codes)):\n",
    "        link = (f\"https://fbref.com/en/squads/{mls_squad_codes['Squad_Code'][squad]}/{mls_year_range[year]}/matchlogs/c22/{cats[5]}/{mls_squad_codes['Text'][squad]}\")\n",
    "        mls_goal_shot_creation_links_2023.append(link)\n",
    "        \n",
    "# Defensive Action Links\n",
    "big_5_defensive_action_links_2023 = []\n",
    "mls_defensive_action_links_2023 = []\n",
    "\n",
    "for year in range(len(year_range)):\n",
    "    for squad in range(len(big5_squad_codes)):\n",
    "        link = (f\"https://fbref.com/en/squads/{big5_squad_codes['Squad_Code'][squad]}/{year_range[year]}/matchlogs/all_comps/{cats[6]}/{big5_squad_codes['Text'][squad]}\")\n",
    "        big_5_defensive_action_links_2023.append(link)\n",
    "        \n",
    "for year in range(len(mls_year_range)):\n",
    "    for squad in range(len(mls_squad_codes)):\n",
    "        link = (f\"https://fbref.com/en/squads/{mls_squad_codes['Squad_Code'][squad]}/{mls_year_range[year]}/matchlogs/c22/{cats[6]}/{mls_squad_codes['Text'][squad]}\")\n",
    "        mls_defensive_action_links_2023.append(link)\n",
    "        \n",
    "# Possession Links\n",
    "big_5_possession_links_2023 = []\n",
    "mls_possession_links_2023 = []\n",
    "\n",
    "for year in range(len(year_range)):\n",
    "    for squad in range(len(big5_squad_codes)):\n",
    "        link = (f\"https://fbref.com/en/squads/{big5_squad_codes['Squad_Code'][squad]}/{year_range[year]}/matchlogs/all_comps/{cats[7]}/{big5_squad_codes['Text'][squad]}\")\n",
    "        big_5_possession_links_2023.append(link)\n",
    "        \n",
    "for year in range(len(mls_year_range)):\n",
    "    for squad in range(len(mls_squad_codes)):\n",
    "        link = (f\"https://fbref.com/en/squads/{mls_squad_codes['Squad_Code'][squad]}/{mls_year_range[year]}/matchlogs/c22/{cats[7]}/{mls_squad_codes['Text'][squad]}\")\n",
    "        mls_possession_links_2023.append(link)\n",
    "        \n",
    "# Misc Links\n",
    "big_5_misc_stats_links_2023 = []\n",
    "mls_misc_stats_links_2023 = []\n",
    "\n",
    "for year in range(len(year_range)):\n",
    "    for squad in range(len(big5_squad_codes)):\n",
    "        link = (f\"https://fbref.com/en/squads/{big5_squad_codes['Squad_Code'][squad]}/{year_range[year]}/matchlogs/all_comps/{cats[8]}/{big5_squad_codes['Text'][squad]}\")\n",
    "        big_5_misc_stats_links_2023.append(link)\n",
    "        \n",
    "for year in range(len(mls_year_range)):\n",
    "    for squad in range(len(mls_squad_codes)):\n",
    "        link = (f\"https://fbref.com/en/squads/{mls_squad_codes['Squad_Code'][squad]}/{mls_year_range[year]}/matchlogs/c22/{cats[8]}/{mls_squad_codes['Text'][squad]}\")\n",
    "        mls_misc_stats_links_2023.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9770c881",
   "metadata": {},
   "outputs": [],
   "source": [
    "mls_misc_stats_links_2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3382cd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(big_5_goalkeeping_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac436a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Team & Opp Dataframes. Big 5 and mls are seperate. \n",
    "big_5_score_df = pd.DataFrame(columns=big_5_score_columns, dtype=object)\n",
    "big_5_shooting_df = pd.DataFrame(columns=big_5_shooting_columns, dtype=object)\n",
    "big_5_goalkeeping_df = pd.DataFrame(columns=big_5_goalkeeping_columns, dtype=object)\n",
    "big_5_passing_df = pd.DataFrame(columns=big_5_passing_columns, dtype=object)\n",
    "big_5_pass_type_df = pd.DataFrame(columns=big_5_pass_type_columns, dtype=object)\n",
    "big_5_goal_shot_creation_df = pd.DataFrame(columns=big_5_goal_shot_creation_columns, dtype=object)\n",
    "big_5_defensive_action_df = pd.DataFrame(columns=big_5_defensive_action_columns, dtype=object)\n",
    "big_5_possession_df = pd.DataFrame(columns=big_5_possession_columns, dtype=object)\n",
    "big_5_misc_stats_df = pd.DataFrame(columns=big_5_misc_stats_columns, dtype=object)\n",
    "\n",
    "big_5_opp_shooting_df = pd.DataFrame(columns=big_5_opp_shooting_columns, dtype=object)\n",
    "big_5_opp_goalkeeping_df = pd.DataFrame(columns=big_5_opp_goalkeeping_columns, dtype=object)\n",
    "big_5_opp_passing_df = pd.DataFrame(columns=big_5_opp_passing_columns, dtype=object)\n",
    "big_5_opp_pass_type_df = pd.DataFrame(columns=big_5_opp_pass_type_columns, dtype=object)\n",
    "big_5_opp_goal_shot_creation_df = pd.DataFrame(columns=big_5_opp_goal_shot_creation_columns, dtype=object)\n",
    "big_5_opp_defensive_action_df = pd.DataFrame(columns=big_5_opp_defensive_action_columns, dtype=object)\n",
    "big_5_opp_possession_df = pd.DataFrame(columns=big_5_opp_possession_columns, dtype=object)\n",
    "big_5_opp_misc_stats_df = pd.DataFrame(columns=big_5_opp_misc_stats_columns, dtype=object)\n",
    "\n",
    "mls_score_df = pd.DataFrame(columns=mls_score_columns, dtype=object)\n",
    "mls_shooting_df = pd.DataFrame(columns=mls_shooting_columns, dtype=object)\n",
    "mls_goalkeeping_df = pd.DataFrame(columns=mls_goalkeeping_columns, dtype=object)\n",
    "mls_passing_df = pd.DataFrame(columns=mls_passing_columns, dtype=object)\n",
    "mls_pass_type_df = pd.DataFrame(columns=mls_pass_type_columns, dtype=object)\n",
    "mls_goal_shot_creation_df = pd.DataFrame(columns=mls_goal_shot_creation_columns, dtype=object)\n",
    "mls_defensive_action_df = pd.DataFrame(columns=mls_defensive_action_columns, dtype=object)\n",
    "mls_possession_df = pd.DataFrame(columns=mls_possession_columns, dtype=object)\n",
    "mls_misc_stats_df = pd.DataFrame(columns=mls_misc_stats_columns, dtype=object)\n",
    "\n",
    "mls_opp_shooting_df = pd.DataFrame(columns=mls_opp_shooting_columns, dtype=object)\n",
    "mls_opp_goalkeeping_df = pd.DataFrame(columns=mls_opp_goalkeeping_columns, dtype=object)\n",
    "mls_opp_passing_df = pd.DataFrame(columns=mls_opp_passing_columns, dtype=object)\n",
    "mls_opp_pass_type_df = pd.DataFrame(columns=mls_opp_pass_type_columns, dtype=object)\n",
    "mls_opp_goal_shot_creation_df = pd.DataFrame(columns=mls_opp_goal_shot_creation_columns, dtype=object)\n",
    "mls_opp_defensive_action_df = pd.DataFrame(columns=mls_opp_defensive_action_columns, dtype=object)\n",
    "mls_opp_possession_df = pd.DataFrame(columns=mls_opp_possession_columns, dtype=object)\n",
    "mls_opp_misc_stats_df = pd.DataFrame(columns=mls_opp_misc_stats_columns, dtype=object)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445de3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big 5 Score Data - collected 3.5\n",
    "i = 0\n",
    "# Loop through each link\n",
    "for link in tqdm.tqdm(big_5_score_links_2023):\n",
    "    time.sleep(3)\n",
    "     \n",
    "    try:\n",
    "        # Use requests to get the HTML content of the page\n",
    "        res = requests.get(big_5_score_links_2023[i])\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        i += 1\n",
    "        # Find the header for team info - this is different than the rest of the stats categories\n",
    "        inner_nav = soup.find('div', id='inner_nav')\n",
    "        header_data = inner_nav.find('a').text\n",
    "\n",
    "\n",
    "        # Find the table with the specified div id\n",
    "        table = soup.find('table', {'id': 'matchlogs_for'})\n",
    "\n",
    "        # Find table body\n",
    "        table_body = table.find('tbody')\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table_body.find_all('tr')\n",
    "\n",
    "        # Initialize an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td','th'])\n",
    "            cols = [col.text.strip() for col in cols]\n",
    "            data.append(cols)\n",
    "\n",
    "        # Create a temporary dataframe from the extracted data\n",
    "        temp_df = pd.DataFrame(data, columns=big_5_score_columns)\n",
    "        temp_df['Team'] = header_data\n",
    "        big_5_score_df = big_5_score_df.append(temp_df, ignore_index=True)\n",
    "        \n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba20d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_5_score_df.to_csv('big_5_score_data_2023.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25312e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mls Score Data - collected 3.5\n",
    "i = 0\n",
    "\n",
    "for link in tqdm.tqdm(mls_score_links_2023):\n",
    "    try:\n",
    "        time.sleep(3)\n",
    "\n",
    "        # Use requests to get the HTML content of the page\n",
    "        res = requests.get(mls_score_links_2023[i])\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        i += 1\n",
    "        # Find the header for team info - this is different than the rest of the stats categories\n",
    "        inner_nav = soup.find('div', id='inner_nav')\n",
    "        header_data = inner_nav.find('a').text\n",
    "\n",
    "\n",
    "        # Find the table with the specified div id\n",
    "        table = soup.find('table', {'id': 'matchlogs_for'})\n",
    "\n",
    "        # Find table body\n",
    "        table_body = table.find('tbody')\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table_body.find_all('tr')\n",
    "\n",
    "        # Initialize an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td','th'])\n",
    "            cols = [col.text.strip() for col in cols]\n",
    "            data.append(cols)\n",
    "\n",
    "        # Create a temporary dataframe from the extracted data\n",
    "        temp_df = pd.DataFrame(data, columns=mls_score_columns)\n",
    "        temp_df['Team'] = header_data\n",
    "        mls_score_df = mls_score_df.append(temp_df, ignore_index=True)\n",
    "    \n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb17187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mls_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ad63c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mls_score_df.to_csv('mls_score_data_2023.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6341495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big 5 Team Shooting Info - collected 3.5\n",
    "i = 0\n",
    "for link in tqdm.tqdm(big_5_shooting_links_2023):\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(big_5_shooting_links_2023[i])\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        i += 1\n",
    "\n",
    "        # Find the header for team info\n",
    "        header_data = soup.find('th', {'data-stat': 'header_for_against'}).text\n",
    "\n",
    "        # Find the table with the specified div id\n",
    "        table = soup.find('table', {'id': 'matchlogs_for'})\n",
    "\n",
    "        # Find table body\n",
    "        table_body = table.find('tbody')\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table_body.find_all('tr')\n",
    "\n",
    "        # Initialize an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td','th'])\n",
    "            cols = [col.text.strip() for col in cols]\n",
    "            data.append(cols)\n",
    "\n",
    "        # Create a temporary dataframe from the extracted data\n",
    "        temp_df = pd.DataFrame(data, columns=big_5_shooting_columns)\n",
    "        temp_df['Team'] = header_data\n",
    "        big_5_shooting_df = big_5_shooting_df.append(temp_df, ignore_index=True)\n",
    "        \n",
    "    except:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa73e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_5_shooting_df.to_csv('big_5_shooting_2023.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3b864f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big 5 Opponent Shooting Info - collected 3.5\n",
    "i = 0\n",
    "for link in tqdm.tqdm(big_5_shooting_links_2023):\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(big_5_shooting_links_2023[i])\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        i += 1\n",
    "\n",
    "        # Find the header for team info\n",
    "        header_data = soup.find('th', {'data-stat': 'header_for_against'}).text\n",
    "\n",
    "        # Find the table with the specified div id\n",
    "        table = soup.find('table', {'id': 'matchlogs_against'})\n",
    "\n",
    "        # Find table body\n",
    "        table_body = table.find('tbody')\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table_body.find_all('tr')\n",
    "\n",
    "        # Initialize an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td','th'])\n",
    "            cols = [col.text.strip() for col in cols]\n",
    "            data.append(cols)\n",
    "\n",
    "        # Create a temporary dataframe from the extracted data\n",
    "        temp_df = pd.DataFrame(data, columns=big_5_opp_shooting_columns)\n",
    "        temp_df['Team'] = header_data\n",
    "        big_5_opp_shooting_df = big_5_opp_shooting_df.append(temp_df, ignore_index=True)\n",
    "        \n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaa0bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_5_opp_shooting_df.to_csv('big_5_opp_shooting_2023.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863cff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLS Team Shooting Info - collected 3.5\n",
    "\n",
    "i = 0\n",
    "for link in tqdm.tqdm(mls_shooting_links_2023):\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(mls_shooting_links_2023[i])\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        i += 1\n",
    "\n",
    "        # Find the header for team info\n",
    "        header_data = soup.find('th', {'data-stat': 'header_for_against'}).text\n",
    "\n",
    "        # Find the table with the specified div id\n",
    "        table = soup.find('table', {'id': 'matchlogs_for'})\n",
    "\n",
    "        # Find table body\n",
    "        table_body = table.find('tbody')\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table_body.find_all('tr')\n",
    "\n",
    "        # Initialize an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td','th'])\n",
    "            cols = [col.text.strip() for col in cols]\n",
    "            data.append(cols)\n",
    "\n",
    "        # Create a temporary dataframe from the extracted data\n",
    "        temp_df = pd.DataFrame(data, columns=mls_shooting_columns)\n",
    "        temp_df['Team'] = header_data\n",
    "        mls_shooting_df = mls_shooting_df.append(temp_df, ignore_index=True)\n",
    "    \n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69584a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "mls_shooting_df.to_csv('mls_shooting_2023.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3c19ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLS Opponent Shooting Info collected 3.5\n",
    "\n",
    "i = 0\n",
    "for link in tqdm.tqdm(mls_shooting_links_2023):\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(mls_shooting_links_2023[i])\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        i += 1\n",
    "\n",
    "        # Find the header for team info\n",
    "        header_data = soup.find('th', {'data-stat': 'header_for_against'}).text\n",
    "\n",
    "        # Find the table with the specified div id\n",
    "        table = soup.find('table', {'id': 'matchlogs_against'})\n",
    "\n",
    "        # Find table body\n",
    "        table_body = table.find('tbody')\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table_body.find_all('tr')\n",
    "\n",
    "        # Initialize an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td','th'])\n",
    "            cols = [col.text.strip() for col in cols]\n",
    "            data.append(cols)\n",
    "\n",
    "        # Create a temporary dataframe from the extracted data\n",
    "        temp_df = pd.DataFrame(data, columns=mls_opp_shooting_columns)\n",
    "        temp_df['Team'] = header_data\n",
    "        mls_opp_shooting_df = mls_opp_shooting_df.append(temp_df, ignore_index=True)\n",
    "    \n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdef24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mls_opp_shooting_df.to_csv('mls_opp_shooting_2023.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f07563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big 5 Goalkeeping Info collected 3.5\n",
    "\n",
    "i = 0\n",
    "for link in tqdm.tqdm(big_5_goalkeeping_links_2023):\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(big_5_goalkeeping_links_2023[i])\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        i += 1\n",
    "\n",
    "        # Find the header for team info\n",
    "        header_data = soup.find('th', {'data-stat': 'header_for_against'}).text\n",
    "\n",
    "        # Find the table with the specified div id\n",
    "        table = soup.find('table', {'id': 'matchlogs_for'})\n",
    "\n",
    "        # Find table body\n",
    "        table_body = table.find('tbody')\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table_body.find_all('tr')\n",
    "\n",
    "        # Initialize an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td','th'])\n",
    "            cols = [col.text.strip() for col in cols]\n",
    "            data.append(cols)\n",
    "\n",
    "        # Create a temporary dataframe from the extracted data\n",
    "        temp_df = pd.DataFrame(data, columns=big_5_goalkeeping_columns)\n",
    "        # This breaks the code for some reason. temp_df['Team'] = header_data\n",
    "        big_5_goalkeeping_df = big_5_goalkeeping_df.append(temp_df, ignore_index=True)\n",
    "        \n",
    "    except:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f177a071",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_5_goalkeeping_df.to_csv('big_5_goalkeeping_2023.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca6d24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big 5 Opponent Goalkeeping Info collected 3.5\n",
    "\n",
    "i = 0\n",
    "for link in tqdm.tqdm(big_5_goalkeeping_links_2023):\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(big_5_goalkeeping_links_2023[i])\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        i += 1\n",
    "\n",
    "        # Find the header for team info\n",
    "        header_data = soup.find('th', {'data-stat': 'header_for_against'}).text\n",
    "\n",
    "        # Find the table with the specified div id\n",
    "        table = soup.find('table', {'id': 'matchlogs_against'})\n",
    "\n",
    "        # Find table body\n",
    "        table_body = table.find('tbody')\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table_body.find_all('tr')\n",
    "\n",
    "        # Initialize an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td','th'])\n",
    "            cols = [col.text.strip() for col in cols]\n",
    "            data.append(cols)\n",
    "\n",
    "        # Create a temporary dataframe from the extracted data\n",
    "        temp_df = pd.DataFrame(data, columns=big_5_opp_goalkeeping_columns)\n",
    "        # This breaks the code for some reason temp_df['Team'] = header_data\n",
    "        big_5_opp_goalkeeping_df = big_5_opp_goalkeeping_df.append(temp_df, ignore_index=True)\n",
    "        \n",
    "    except:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48b07bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_5_opp_goalkeeping_df.to_csv('big_5_opp_goalkeeping_2023.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daea7c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLS Goalkeeping Info collected 3.5\n",
    "\n",
    "i = 0\n",
    "for link in tqdm.tqdm(mls_goalkeeping_links_2023):\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(mls_goalkeeping_links_2023[i])\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        i += 1\n",
    "\n",
    "        # Find the header for team info\n",
    "        header_data = soup.find('th', {'data-stat': 'header_for_against'}).text\n",
    "\n",
    "        # Find the table with the specified div id\n",
    "        table = soup.find('table', {'id': 'matchlogs_for'})\n",
    "\n",
    "        # Find table body\n",
    "        table_body = table.find('tbody')\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table_body.find_all('tr')\n",
    "\n",
    "        # Initialize an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td','th'])\n",
    "            cols = [col.text.strip() for col in cols]\n",
    "            data.append(cols)\n",
    "\n",
    "        # Create a temporary dataframe from the extracted data\n",
    "        temp_df = pd.DataFrame(data, columns=mls_goalkeeping_columns)\n",
    "        # This breaks the code for some reason temp_df['Team'] = header_data\n",
    "        mls_goalkeeping_df = mls_goalkeeping_df.append(temp_df, ignore_index=True)\n",
    "        \n",
    "    except:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9efeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mls_goalkeeping_df.to_csv('mls_goalkeeping_2023.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfa65f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLS Opponent Goalkeeping Info collected 3.5\n",
    "\n",
    "i = 0\n",
    "for link in tqdm.tqdm(mls_goalkeeping_links_2023):\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(mls_goalkeeping_links_2023[i])\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        i += 1\n",
    "\n",
    "        # Find the header for team info\n",
    "        header_data = soup.find('th', {'data-stat': 'header_for_against'}).text\n",
    "\n",
    "        # Find the table with the specified div id\n",
    "        table = soup.find('table', {'id': 'matchlogs_against'})\n",
    "\n",
    "        # Find table body\n",
    "        table_body = table.find('tbody')\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table_body.find_all('tr')\n",
    "\n",
    "        # Initialize an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td','th'])\n",
    "            cols = [col.text.strip() for col in cols]\n",
    "            data.append(cols)\n",
    "\n",
    "        # Create a temporary dataframe from the extracted data\n",
    "        temp_df = pd.DataFrame(data, columns=mls_opp_goalkeeping_columns)\n",
    "        #temp_df['Team'] = header_data\n",
    "        mls_opp_goalkeeping_df = mls_opp_goalkeeping_df.append(temp_df, ignore_index=True)\n",
    "        \n",
    "    except:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978ed96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mls_opp_goalkeeping_df.to_csv('mls_opp_goalkeeping_2023.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b16f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big 5 Team Passing Info - collected 3.5\n",
    "\n",
    "i = 0\n",
    "for link in tqdm.tqdm(big_5_passing_links_2023):\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(big_5_passing_links_2023[i])\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        i += 1\n",
    "\n",
    "        # Find the header for team info\n",
    "        header_data = soup.find('th', {'data-stat': 'header_for_against'}).text\n",
    "\n",
    "        # Find the table with the specified div id\n",
    "        table = soup.find('table', {'id': 'matchlogs_for'})\n",
    "\n",
    "        # Find table body\n",
    "        table_body = table.find('tbody')\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table_body.find_all('tr')\n",
    "\n",
    "        # Initialize an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td','th'])\n",
    "            cols = [col.text.strip() for col in cols]\n",
    "            data.append(cols)\n",
    "\n",
    "        # Create a temporary dataframe from the extracted data\n",
    "        temp_df = pd.DataFrame(data, columns=big_5_passing_columns)\n",
    "        temp_df['Team'] = header_data\n",
    "        big_5_passing_df = big_5_passing_df.append(temp_df, ignore_index=True)\n",
    "        \n",
    "    except:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14c679e",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_5_passing_df.to_csv('big_5_passing_data_2023.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390efd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big 5 Opponent Passing Info - collected 3.5\n",
    "\n",
    "i = 0\n",
    "for link in tqdm.tqdm(big_5_passing_links_2023):\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(big_5_passing_links_2023[i])\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        i += 1\n",
    "\n",
    "        # Find the header for team info\n",
    "        header_data = soup.find('th', {'data-stat': 'header_for_against'}).text\n",
    "\n",
    "        # Find the table with the specified div id\n",
    "        table = soup.find('table', {'id': 'matchlogs_against'})\n",
    "\n",
    "        # Find table body\n",
    "        table_body = table.find('tbody')\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table_body.find_all('tr')\n",
    "\n",
    "        # Initialize an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td','th'])\n",
    "            cols = [col.text.strip() for col in cols]\n",
    "            data.append(cols)\n",
    "\n",
    "        # Create a temporary dataframe from the extracted data\n",
    "        temp_df = pd.DataFrame(data, columns=big_5_opp_passing_columns)\n",
    "        temp_df['Team'] = header_data\n",
    "        big_5_opp_passing_df = big_5_opp_passing_df.append(temp_df, ignore_index=True)\n",
    "        \n",
    "    except:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b3a4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_5_opp_passing_df.to_csv('big_5_opp_passing_data_2023.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fcb8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLS Passing Info - collected 3.5\n",
    "\n",
    "i = 0\n",
    "for link in tqdm.tqdm(mls_passing_links_2023):\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(mls_passing_links_2023[i])\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        i += 1\n",
    "\n",
    "        # Find the header for team info\n",
    "        header_data = soup.find('th', {'data-stat': 'header_for_against'}).text\n",
    "\n",
    "        # Find the table with the specified div id\n",
    "        table = soup.find('table', {'id': 'matchlogs_for'})\n",
    "\n",
    "        # Find table body\n",
    "        table_body = table.find('tbody')\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table_body.find_all('tr')\n",
    "\n",
    "        # Initialize an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td','th'])\n",
    "            cols = [col.text.strip() for col in cols]\n",
    "            data.append(cols)\n",
    "\n",
    "        # Create a temporary dataframe from the extracted data\n",
    "        temp_df = pd.DataFrame(data, columns=mls_passing_columns)\n",
    "        temp_df['Team'] = header_data\n",
    "        mls_passing_df = mls_passing_df.append(temp_df, ignore_index=True)\n",
    "        \n",
    "    except:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf4d4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mls_passing_df.to_csv(\"mls_passing_2023.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af0a5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLS Opponent Passing Info - collected 3.5\n",
    "\n",
    "i = 0\n",
    "for link in tqdm.tqdm(mls_passing_links_2023):\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(mls_passing_links_2023[i])\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        i += 1\n",
    "\n",
    "        # Find the header for team info\n",
    "        header_data = soup.find('th', {'data-stat': 'header_for_against'}).text\n",
    "\n",
    "        # Find the table with the specified div id\n",
    "        table = soup.find('table', {'id': 'matchlogs_against'})\n",
    "\n",
    "        # Find table body\n",
    "        table_body = table.find('tbody')\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table_body.find_all('tr')\n",
    "\n",
    "        # Initialize an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td','th'])\n",
    "            cols = [col.text.strip() for col in cols]\n",
    "            data.append(cols)\n",
    "\n",
    "        # Create a temporary dataframe from the extracted data\n",
    "        temp_df = pd.DataFrame(data, columns=mls_opp_passing_columns)\n",
    "        temp_df['Team'] = header_data\n",
    "        mls_opp_passing_df = mls_opp_passing_df.append(temp_df, ignore_index=True)\n",
    "        \n",
    "    except:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ed754a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mls_opp_passing_df.to_csv(\"mls_opp_passing_2023.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cfe220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big 5 Team Pass Type Info collected 3.5\n",
    "\n",
    "i = 0\n",
    "for link in tqdm.tqdm(big_5_pass_type_links_2023):\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(big_5_pass_type_links_2023[i])\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        i += 1\n",
    "\n",
    "        # Find the header for team info\n",
    "        header_data = soup.find('th', {'data-stat': 'header_for_against'}).text\n",
    "\n",
    "        # Find the table with the specified div id\n",
    "        table = soup.find('table', {'id': 'matchlogs_for'})\n",
    "\n",
    "        # Find table body\n",
    "        table_body = table.find('tbody')\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table_body.find_all('tr')\n",
    "\n",
    "        # Initialize an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td','th'])\n",
    "            cols = [col.text.strip() for col in cols]\n",
    "            data.append(cols)\n",
    "\n",
    "        # Create a temporary dataframe from the extracted data\n",
    "        temp_df = pd.DataFrame(data, columns=big_5_pass_type_columns)\n",
    "        temp_df['Team'] = header_data\n",
    "        big_5_pass_type_df = big_5_pass_type_df.append(temp_df, ignore_index=True)\n",
    "        \n",
    "    except:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa95b4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_5_pass_type_df.to_csv('big_5_pass_type_2023.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c727e49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big 5 Opponent Pass Type Info collected 3.5\n",
    "\n",
    "i = 0\n",
    "for link in tqdm.tqdm(big_5_pass_type_links_2023):\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(big_5_pass_type_links_2023[i])\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        i += 1\n",
    "\n",
    "        # Find the header for team info\n",
    "        header_data = soup.find('th', {'data-stat': 'header_for_against'}).text\n",
    "\n",
    "        # Find the table with the specified div id\n",
    "        table = soup.find('table', {'id': 'matchlogs_against'})\n",
    "\n",
    "        # Find table body\n",
    "        table_body = table.find('tbody')\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table_body.find_all('tr')\n",
    "\n",
    "        # Initialize an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td','th'])\n",
    "            cols = [col.text.strip() for col in cols]\n",
    "            data.append(cols)\n",
    "\n",
    "        # Create a temporary dataframe from the extracted data\n",
    "        temp_df = pd.DataFrame(data, columns=big_5_opp_pass_type_columns)\n",
    "        temp_df['Team'] = header_data\n",
    "        big_5_opp_pass_type_df = big_5_opp_pass_type_df.append(temp_df, ignore_index=True)\n",
    "        \n",
    "    except:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669fbddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_5_opp_pass_type_df.to_csv('big_5_opp_pass_type_2023.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53689b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLS Pass Type Info Already collected 3.5\n",
    "\n",
    "i = 0\n",
    "for link in tqdm.tqdm(mls_pass_type_links_2023):\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(mls_pass_type_links_2023[i])\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        i += 1\n",
    "\n",
    "        # Find the header for team info\n",
    "        header_data = soup.find('th', {'data-stat': 'header_for_against'}).text\n",
    "\n",
    "        # Find the table with the specified div id\n",
    "        table = soup.find('table', {'id': 'matchlogs_for'})\n",
    "\n",
    "        # Find table body\n",
    "        table_body = table.find('tbody')\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table_body.find_all('tr')\n",
    "\n",
    "        # Initialize an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td','th'])\n",
    "            cols = [col.text.strip() for col in cols]\n",
    "            data.append(cols)\n",
    "\n",
    "        # Create a temporary dataframe from the extracted data\n",
    "        temp_df = pd.DataFrame(data, columns=mls_pass_type_columns)\n",
    "        temp_df['Team'] = header_data\n",
    "        mls_pass_type_df = mls_pass_type_df.append(temp_df, ignore_index=True)\n",
    "        \n",
    "    except:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38d3363",
   "metadata": {},
   "outputs": [],
   "source": [
    "mls_pass_type_df.to_csv(\"mls_team_pass_type_2023.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0683472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLS Opponent Pass Type Info Already collected 3.5\n",
    "\n",
    "i = 0\n",
    "for link in tqdm.tqdm(mls_pass_type_links_2023):\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(mls_pass_type_links_2023[i])\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        i += 1\n",
    "\n",
    "        # Find the header for team info\n",
    "        header_data = soup.find('th', {'data-stat': 'header_for_against'}).text\n",
    "\n",
    "        # Find the table with the specified div id\n",
    "        table = soup.find('table', {'id': 'matchlogs_against'})\n",
    "\n",
    "        # Find table body\n",
    "        table_body = table.find('tbody')\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table_body.find_all('tr')\n",
    "\n",
    "        # Initialize an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td','th'])\n",
    "            cols = [col.text.strip() for col in cols]\n",
    "            data.append(cols)\n",
    "\n",
    "        # Create a temporary dataframe from the extracted data\n",
    "        temp_df = pd.DataFrame(data, columns=mls_opp_pass_type_columns)\n",
    "        temp_df['Team'] = header_data\n",
    "        mls_opp_pass_type_df = mls_opp_pass_type_df.append(temp_df, ignore_index=True)\n",
    "        \n",
    "    except:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9ab96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mls_opp_pass_type_df.to_csv(\"mls_opp_pass_type_2023.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb4ee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big 5 Team Goal Shot Creation Info collected 3.5\n",
    "\n",
    "i = 0\n",
    "for link in tqdm.tqdm(big_5_goal_shot_creation_links_2023):\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(big_5_goal_shot_creation_links_2023[i])\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        i += 1\n",
    "\n",
    "        # Find the header for team info\n",
    "        header_data = soup.find('th', {'data-stat': 'header_for_against'}).text\n",
    "\n",
    "        # Find the table with the specified div id\n",
    "        table = soup.find('table', {'id': 'matchlogs_for'})\n",
    "\n",
    "        # Find table body\n",
    "        table_body = table.find('tbody')\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table_body.find_all('tr')\n",
    "\n",
    "        # Initialize an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td','th'])\n",
    "            cols = [col.text.strip() for col in cols]\n",
    "            data.append(cols)\n",
    "\n",
    "        # Create a temporary dataframe from the extracted data\n",
    "        temp_df = pd.DataFrame(data, columns=big_5_goal_shot_creation_columns)\n",
    "        temp_df['Team'] = header_data\n",
    "        big_5_goal_shot_creation_df = big_5_goal_shot_creation_df.append(temp_df, ignore_index=True)\n",
    "        \n",
    "    except:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8d60b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_5_goal_shot_creation_df.to_csv('big_5_sca_data_2023.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a69e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big 5 Opponent Goal Shot Creation Info collected 3.5\n",
    "\n",
    "i = 0\n",
    "for link in tqdm.tqdm(big_5_goal_shot_creation_links_2023):\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(big_5_goal_shot_creation_links_2023[i])\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        i += 1\n",
    "\n",
    "        # Find the header for team info\n",
    "        header_data = soup.find('th', {'data-stat': 'header_for_against'}).text\n",
    "\n",
    "        # Find the table with the specified div id\n",
    "        table = soup.find('table', {'id': 'matchlogs_against'})\n",
    "\n",
    "        # Find table body\n",
    "        table_body = table.find('tbody')\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table_body.find_all('tr')\n",
    "\n",
    "        # Initialize an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td','th'])\n",
    "            cols = [col.text.strip() for col in cols]\n",
    "            data.append(cols)\n",
    "\n",
    "        # Create a temporary dataframe from the extracted data\n",
    "        temp_df = pd.DataFrame(data, columns=big_5_opp_goal_shot_creation_columns)\n",
    "        temp_df['Team'] = header_data\n",
    "        big_5_opp_goal_shot_creation_df = big_5_opp_goal_shot_creation_df.append(temp_df, ignore_index=True)\n",
    "        \n",
    "    except:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aac835",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_5_opp_goal_shot_creation_df.to_csv('big_5_opp_gca_2023.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a6a468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLS Goal Shot Creation Info collected 3.5\n",
    "\n",
    "i = 0\n",
    "for link in tqdm.tqdm(mls_goal_shot_creation_links_2023):\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(mls_goal_shot_creation_links_2023[i])\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        i += 1\n",
    "\n",
    "        # Find the header for team info\n",
    "        header_data = soup.find('th', {'data-stat': 'header_for_against'}).text\n",
    "\n",
    "        # Find the table with the specified div id\n",
    "        table = soup.find('table', {'id': 'matchlogs_for'})\n",
    "\n",
    "        # Find table body\n",
    "        table_body = table.find('tbody')\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table_body.find_all('tr')\n",
    "\n",
    "        # Initialize an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td','th'])\n",
    "            cols = [col.text.strip() for col in cols]\n",
    "            data.append(cols)\n",
    "\n",
    "        # Create a temporary dataframe from the extracted data\n",
    "        temp_df = pd.DataFrame(data, columns=mls_goal_shot_creation_columns)\n",
    "        temp_df['Team'] = header_data\n",
    "        mls_goal_shot_creation_df = mls_goal_shot_creation_df.append(temp_df, ignore_index=True)\n",
    "        \n",
    "    except:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1d09ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "mls_goal_shot_creation_df.to_csv(\"mls_gca_2023.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa908ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLS Opponent Goal Shot Creation Info collected 3.5\n",
    "\n",
    "i = 0\n",
    "for link in tqdm.tqdm(mls_goal_shot_creation_links_2023):\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(mls_goal_shot_creation_links_2023[i])\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        i += 1\n",
    "\n",
    "        # Find the header for team info\n",
    "        header_data = soup.find('th', {'data-stat': 'header_for_against'}).text\n",
    "\n",
    "        # Find the table with the specified div id\n",
    "        table = soup.find('table', {'id': 'matchlogs_against'})\n",
    "\n",
    "        # Find table body\n",
    "        table_body = table.find('tbody')\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table_body.find_all('tr')\n",
    "\n",
    "        # Initialize an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td','th'])\n",
    "            cols = [col.text.strip() for col in cols]\n",
    "            data.append(cols)\n",
    "\n",
    "        # Create a temporary dataframe from the extracted data\n",
    "        temp_df = pd.DataFrame(data, columns=mls_opp_goal_shot_creation_columns)\n",
    "        temp_df['Team'] = header_data\n",
    "        mls_opp_goal_shot_creation_df = mls_opp_goal_shot_creation_df.append(temp_df, ignore_index=True)\n",
    "        \n",
    "    except:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832d0ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mls_opp_goal_shot_creation_df.to_csv(\"mls_opp_gca_2023.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b0bd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big 5 Team Defensive Info collected 3.5\n",
    "\n",
    "i = 0\n",
    "for link in tqdm.tqdm(big_5_defensive_action_links_2023):\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(big_5_defensive_action_links_2023[i])\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        i += 1\n",
    "\n",
    "        # Find the header for team info\n",
    "        header_data = soup.find('th', {'data-stat': 'header_for_against'}).text\n",
    "\n",
    "        # Find the table with the specified div id\n",
    "        table = soup.find('table', {'id': 'matchlogs_for'})\n",
    "\n",
    "        # Find table body\n",
    "        table_body = table.find('tbody')\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table_body.find_all('tr')\n",
    "\n",
    "        # Initialize an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td','th'])\n",
    "            cols = [col.text.strip() for col in cols]\n",
    "            data.append(cols)\n",
    "\n",
    "        # Create a temporary dataframe from the extracted data\n",
    "        temp_df = pd.DataFrame(data, columns=big_5_defensive_action_columns)\n",
    "        temp_df['Team'] = header_data\n",
    "        big_5_defensive_action_df = big_5_defensive_action_df.append(temp_df, ignore_index=True)\n",
    "        \n",
    "    except:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b70a956",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_5_defensive_action_df.to_csv('big_5_defense_2023.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0e2107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big 5 Opponent Defense Info collected 3.5\n",
    "\n",
    "i = 0\n",
    "for link in tqdm.tqdm(big_5_defensive_action_links_2023):\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(big_5_defensive_action_links_2023[i])\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        i += 1\n",
    "\n",
    "        # Find the header for team info\n",
    "        header_data = soup.find('th', {'data-stat': 'header_for_against'}).text\n",
    "\n",
    "        # Find the table with the specified div id\n",
    "        table = soup.find('table', {'id': 'matchlogs_against'})\n",
    "\n",
    "        # Find table body\n",
    "        table_body = table.find('tbody')\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table_body.find_all('tr')\n",
    "\n",
    "        # Initialize an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td','th'])\n",
    "            cols = [col.text.strip() for col in cols]\n",
    "            data.append(cols)\n",
    "\n",
    "        # Create a temporary dataframe from the extracted data\n",
    "        temp_df = pd.DataFrame(data, columns=big_5_opp_defensive_action_columns)\n",
    "        temp_df['Team'] = header_data\n",
    "        big_5_opp_defensive_action_df = big_5_opp_defensive_action_df.append(temp_df, ignore_index=True)\n",
    "        \n",
    "    except:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db628e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_5_opp_defensive_action_df.to_csv('big_5_opp_defense_2023.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7925b87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLS Defense Info collected 3.5\n",
    "\n",
    "i = 0\n",
    "for link in tqdm.tqdm(mls_defensive_action_links_2023):\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(mls_defensive_action_links_2023[i])\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        i += 1\n",
    "\n",
    "        # Find the header for team info\n",
    "        header_data = soup.find('th', {'data-stat': 'header_for_against'}).text\n",
    "\n",
    "        # Find the table with the specified div id\n",
    "        table = soup.find('table', {'id': 'matchlogs_for'})\n",
    "\n",
    "        # Find table body\n",
    "        table_body = table.find('tbody')\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table_body.find_all('tr')\n",
    "\n",
    "        # Initialize an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td','th'])\n",
    "            cols = [col.text.strip() for col in cols]\n",
    "            data.append(cols)\n",
    "\n",
    "        # Create a temporary dataframe from the extracted data\n",
    "        temp_df = pd.DataFrame(data, columns=mls_defensive_action_columns)\n",
    "        temp_df['Team'] = header_data\n",
    "        mls_defensive_action_df = mls_defensive_action_df.append(temp_df, ignore_index=True)\n",
    "        \n",
    "    except:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13db14ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "mls_defensive_action_df.to_csv('mls_team_defense_2023.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6e30d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLS Opponent Defense Info collected 3.5\n",
    "\n",
    "i = 0\n",
    "for link in tqdm.tqdm(mls_defensive_action_links_2023):\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(mls_defensive_action_links_2023[i])\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        i += 1\n",
    "\n",
    "        # Find the header for team info\n",
    "        header_data = soup.find('th', {'data-stat': 'header_for_against'}).text\n",
    "\n",
    "        # Find the table with the specified div id\n",
    "        table = soup.find('table', {'id': 'matchlogs_against'})\n",
    "\n",
    "        # Find table body\n",
    "        table_body = table.find('tbody')\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table_body.find_all('tr')\n",
    "\n",
    "        # Initialize an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td','th'])\n",
    "            cols = [col.text.strip() for col in cols]\n",
    "            data.append(cols)\n",
    "\n",
    "        # Create a temporary dataframe from the extracted data\n",
    "        temp_df = pd.DataFrame(data, columns=mls_opp_defensive_action_columns)\n",
    "        temp_df['Team'] = header_data\n",
    "        mls_opp_defensive_action_df = mls_opp_defensive_action_df.append(temp_df, ignore_index=True)\n",
    "        \n",
    "    except:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07511b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mls_opp_defensive_action_df.to_csv('mls_opp_defense_2023.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27259a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big 5 Team Possession Info collected 3.5\n",
    "\n",
    "i = 0\n",
    "for link in tqdm.tqdm(big_5_possession_links_2023):\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(big_5_possession_links_2023[i])\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        i += 1\n",
    "\n",
    "        # Find the header for team info\n",
    "        header_data = soup.find('th', {'data-stat': 'header_for_against'}).text\n",
    "\n",
    "        # Find the table with the specified div id\n",
    "        table = soup.find('table', {'id': 'matchlogs_for'})\n",
    "\n",
    "        # Find table body\n",
    "        table_body = table.find('tbody')\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table_body.find_all('tr')\n",
    "\n",
    "        # Initialize an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td','th'])\n",
    "            cols = [col.text.strip() for col in cols]\n",
    "            data.append(cols)\n",
    "\n",
    "        # Create a temporary dataframe from the extracted data\n",
    "        temp_df = pd.DataFrame(data, columns=big_5_possession_columns)\n",
    "        temp_df['Team'] = header_data\n",
    "        big_5_possession_df = big_5_possession_df.append(temp_df, ignore_index=True)\n",
    "        \n",
    "    except:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b76d3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_5_possession_df.to_csv('big_5_team_possession_2023.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2e095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big 5 Opponent Possession Info collected 3.5\n",
    "\n",
    "i = 0\n",
    "for link in tqdm.tqdm(big_5_possession_links_2023):\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(big_5_possession_links_2023[i])\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        i += 1\n",
    "\n",
    "        # Find the header for team info\n",
    "        header_data = soup.find('th', {'data-stat': 'header_for_against'}).text\n",
    "\n",
    "        # Find the table with the specified div id\n",
    "        table = soup.find('table', {'id': 'matchlogs_against'})\n",
    "\n",
    "        # Find table body\n",
    "        table_body = table.find('tbody')\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table_body.find_all('tr')\n",
    "\n",
    "        # Initialize an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td','th'])\n",
    "            cols = [col.text.strip() for col in cols]\n",
    "            data.append(cols)\n",
    "\n",
    "        # Create a temporary dataframe from the extracted data\n",
    "        temp_df = pd.DataFrame(data, columns=big_5_opp_possession_columns)\n",
    "        temp_df['Team'] = header_data\n",
    "        big_5_opp_possession_df = big_5_opp_possession_df.append(temp_df, ignore_index=True)\n",
    "        \n",
    "    except:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b1cfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_5_opp_possession_df.to_csv('big_5_opp_possession_2023.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d89b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLS Possession Info collected 3.5\n",
    "\n",
    "i = 0\n",
    "for link in tqdm.tqdm(mls_possession_links_2023):\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(mls_possession_links_2023[i])\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        i += 1\n",
    "\n",
    "        # Find the header for team info\n",
    "        header_data = soup.find('th', {'data-stat': 'header_for_against'}).text\n",
    "\n",
    "        # Find the table with the specified div id\n",
    "        table = soup.find('table', {'id': 'matchlogs_for'})\n",
    "\n",
    "        # Find table body\n",
    "        table_body = table.find('tbody')\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table_body.find_all('tr')\n",
    "\n",
    "        # Initialize an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td','th'])\n",
    "            cols = [col.text.strip() for col in cols]\n",
    "            data.append(cols)\n",
    "\n",
    "        # Create a temporary dataframe from the extracted data\n",
    "        temp_df = pd.DataFrame(data, columns=mls_possession_columns)\n",
    "        temp_df['Team'] = header_data\n",
    "        mls_possession_df = mls_possession_df.append(temp_df, ignore_index=True)\n",
    "        \n",
    "    except:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5647ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "mls_possession_df.to_csv('mls_poss_2023.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329e4f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLS Opponent Possession Info collected 3.5\n",
    "\n",
    "i = 0\n",
    "for link in tqdm.tqdm(mls_possession_links_2023):\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(mls_possession_links_2023[i])\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        i += 1\n",
    "\n",
    "        # Find the header for team info\n",
    "        header_data = soup.find('th', {'data-stat': 'header_for_against'}).text\n",
    "\n",
    "        # Find the table with the specified div id\n",
    "        table = soup.find('table', {'id': 'matchlogs_against'})\n",
    "\n",
    "        # Find table body\n",
    "        table_body = table.find('tbody')\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table_body.find_all('tr')\n",
    "\n",
    "        # Initialize an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td','th'])\n",
    "            cols = [col.text.strip() for col in cols]\n",
    "            data.append(cols)\n",
    "\n",
    "        # Create a temporary dataframe from the extracted data\n",
    "        temp_df = pd.DataFrame(data, columns=mls_opp_possession_columns)\n",
    "        temp_df['Team'] = header_data\n",
    "        mls_opp_possession_df = mls_opp_possession_df.append(temp_df, ignore_index=True)\n",
    "        \n",
    "    except:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919a1fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mls_opp_possession_df.to_csv('mls_opp_poss_2023.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f565382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big 5 Team Misc Info collected 3.5\n",
    "\n",
    "i = 0\n",
    "for link in tqdm.tqdm(big_5_misc_stats_links_2023):\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(big_5_misc_stats_links_2023[i])\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        i += 1\n",
    "\n",
    "        # Find the header for team info\n",
    "        header_data = soup.find('th', {'data-stat': 'header_for_against'}).text\n",
    "\n",
    "        # Find the table with the specified div id\n",
    "        table = soup.find('table', {'id': 'matchlogs_for'})\n",
    "\n",
    "        # Find table body\n",
    "        table_body = table.find('tbody')\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table_body.find_all('tr')\n",
    "\n",
    "        # Initialize an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td','th'])\n",
    "            cols = [col.text.strip() for col in cols]\n",
    "            data.append(cols)\n",
    "\n",
    "        # Create a temporary dataframe from the extracted data\n",
    "        temp_df = pd.DataFrame(data, columns=big_5_misc_stats_columns)\n",
    "        temp_df['Team'] = header_data\n",
    "        big_5_misc_stats_df = big_5_misc_stats_df.append(temp_df, ignore_index=True)\n",
    "        \n",
    "    except:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4fc601",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_5_misc_stats_df.to_csv('big_5_misc_2023.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab6f79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big 5 Opponent Misc Info collected 3.5\n",
    "\n",
    "i = 0\n",
    "for link in tqdm.tqdm(big_5_misc_stats_links_2023):\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(big_5_misc_stats_links_2023[i])\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        i += 1\n",
    "\n",
    "        # Find the header for team info\n",
    "        header_data = soup.find('th', {'data-stat': 'header_for_against'}).text\n",
    "\n",
    "        # Find the table with the specified div id\n",
    "        table = soup.find('table', {'id': 'matchlogs_against'})\n",
    "\n",
    "        # Find table body\n",
    "        table_body = table.find('tbody')\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table_body.find_all('tr')\n",
    "\n",
    "        # Initialize an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td','th'])\n",
    "            cols = [col.text.strip() for col in cols]\n",
    "            data.append(cols)\n",
    "\n",
    "        # Create a temporary dataframe from the extracted data\n",
    "        temp_df = pd.DataFrame(data, columns=big_5_opp_misc_stats_columns)\n",
    "        temp_df['Team'] = header_data\n",
    "        big_5_opp_misc_stats_df = big_5_opp_misc_stats_df.append(temp_df, ignore_index=True)\n",
    "        \n",
    "    except:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdb7a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_5_opp_misc_stats_df.to_csv('big_5_opp_misc_2023.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b3d36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLS Misc Info collected 3.5\n",
    "\n",
    "i = 0\n",
    "for link in tqdm.tqdm(mls_misc_stats_links_2023):\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(mls_misc_stats_links_2023[i])\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        i += 1\n",
    "\n",
    "        # Find the header for team info\n",
    "        header_data = soup.find('th', {'data-stat': 'header_for_against'}).text\n",
    "\n",
    "        # Find the table with the specified div id\n",
    "        table = soup.find('table', {'id': 'matchlogs_for'})\n",
    "\n",
    "        # Find table body\n",
    "        table_body = table.find('tbody')\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table_body.find_all('tr')\n",
    "\n",
    "        # Initialize an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td','th'])\n",
    "            cols = [col.text.strip() for col in cols]\n",
    "            data.append(cols)\n",
    "\n",
    "        # Create a temporary dataframe from the extracted data\n",
    "        temp_df = pd.DataFrame(data, columns=mls_misc_stats_columns)\n",
    "        temp_df['Team'] = header_data\n",
    "        mls_misc_stats_df = mls_misc_stats_df.append(temp_df, ignore_index=True)\n",
    "        \n",
    "    except:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b29b50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mls_misc_stats_df.to_csv('mls_misc_2023.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88d8257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLS Opponent Misc Info collected 3.5\n",
    "\n",
    "i = 0\n",
    "for link in tqdm.tqdm(mls_misc_stats_links_2023):\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(mls_misc_stats_links_2023[i])\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        i += 1\n",
    "\n",
    "        # Find the header for team info\n",
    "        header_data = soup.find('th', {'data-stat': 'header_for_against'}).text\n",
    "\n",
    "        # Find the table with the specified div id\n",
    "        table = soup.find('table', {'id': 'matchlogs_against'})\n",
    "\n",
    "        # Find table body\n",
    "        table_body = table.find('tbody')\n",
    "\n",
    "        # Find all the rows in the table\n",
    "        rows = table_body.find_all('tr')\n",
    "\n",
    "        # Initialize an empty list to store the data\n",
    "        data = []\n",
    "\n",
    "        # Loop through each row and extract the data\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td','th'])\n",
    "            cols = [col.text.strip() for col in cols]\n",
    "            data.append(cols)\n",
    "\n",
    "        # Create a temporary dataframe from the extracted data\n",
    "        temp_df = pd.DataFrame(data, columns=mls_opp_misc_stats_columns)\n",
    "        temp_df['Team'] = header_data\n",
    "        mls_opp_misc_stats_df = mls_opp_misc_stats_df.append(temp_df, ignore_index=True)\n",
    "        \n",
    "    except:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa52b3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mls_opp_misc_stats_df.to_csv('mls_opp_misc_2023.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
